{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hsUWcfin6ZlT"
      },
      "source": [
        "### Implementacja prostej sieci neuronowej\n",
        "\n",
        "##### Kroki:\n",
        "    1. Zainicjowanie parametrów sieci\n",
        "    2. Propagacja wprzód\n",
        "    3. Obliczenie błędu predykcji\n",
        "    4. Propagacja wsteczna (uczenie modelu)\n",
        "    5. Test działania modelu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XkDZb9Nt04rg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.array([1.4, 0.7])\n",
        "y_true = np.array([1.8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.4, 0.7])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.8])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zc5rkn_m1IjB"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    W1 = np.random.rand(n_h, n_x)\n",
        "    W2 = np.random.rand(n_h, n_y)\n",
        "    return W1, W2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "G43XdqDa1exJ"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(X, W1, W2):\n",
        "    H1 = np.dot(X, W1)\n",
        "    y_pred = np.dot(H1, W2)\n",
        "    return H1, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XbaAd2jt1v8t"
      },
      "outputs": [],
      "source": [
        "def calculate_error(y_pred, y_true):\n",
        "    return y_pred - y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "U3QFBEn_14Lj"
      },
      "outputs": [],
      "source": [
        "def predict(X, W1, W2):\n",
        "    _, y_pred = forward_propagation(X, W1, W2)\n",
        "    return y_pred[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rClKyNy02CnX"
      },
      "outputs": [],
      "source": [
        "def backpropagation(X, W1, W2, learning_rate, iters=1000, precision=0.0000001):\n",
        "\n",
        "    H1, y_pred = forward_propagation(X, W1, W2)\n",
        "    train_loss = []\n",
        "\n",
        "    for i in range(iters):\n",
        "        error = calculate_error(y_pred, y_true)\n",
        "        W2 = W2 - learning_rate * error * H1.T \n",
        "        W1 = W1 - learning_rate * error * np.dot(X.T, W2.T)\n",
        "\n",
        "        y_pred = predict(X, W1, W2)\n",
        "        print(f'Iter #{i}: y_pred {y_pred}: loss: {abs(calculate_error(y_pred, y_true[0]))}')\n",
        "        train_loss.append(abs(calculate_error(y_pred, y_true[0])))\n",
        "\n",
        "        if abs(error) < precision:\n",
        "            break\n",
        "\n",
        "    return W1, W2, train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "phVhArP520f3"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "\n",
        "    W1, W2 = initialize_parameters(2, 2, 1)\n",
        "    \n",
        "    W1, W2, train_loss = backpropagation(X, W1, W2, 0.01)\n",
        "\n",
        "    model = {'W1': W1, 'W2': W2, 'train_loss': train_loss}\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "Sg7M4xDv3JFT",
        "outputId": "19f5d852-3b8d-4c06-b7a6-b4dc35c10e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter #0: y_pred 0.47775082418068876: loss: 1.3222491758193113\n",
            "Iter #1: y_pred 0.5203197164995874: loss: 1.2796802835004126\n",
            "Iter #2: y_pred 0.5624603087798719: loss: 1.2375396912201282\n",
            "Iter #3: y_pred 0.6041630898849911: loss: 1.195836910115009\n",
            "Iter #4: y_pred 0.6454096471221384: loss: 1.1545903528778616\n",
            "Iter #5: y_pred 0.6861742668168541: loss: 1.113825733183146\n",
            "Iter #6: y_pred 0.7264254411736586: loss: 1.0735745588263415\n",
            "Iter #7: y_pred 0.7661272789328607: loss: 1.0338727210671395\n",
            "Iter #8: y_pred 0.8052408159236343: loss: 0.9947591840763658\n",
            "Iter #9: y_pred 0.8437252210745323: loss: 0.9562747789254677\n",
            "Iter #10: y_pred 0.8815388936200005: loss: 0.9184611063799996\n",
            "Iter #11: y_pred 0.9186404479839603: loss: 0.8813595520160398\n",
            "Iter #12: y_pred 0.9549895839833127: loss: 0.8450104160166874\n",
            "Iter #13: y_pred 0.9905478414374655: loss: 0.8094521585625345\n",
            "Iter #14: y_pred 1.0252792398682151: loss: 0.7747207601317849\n",
            "Iter #15: y_pred 1.0591508056150682: loss: 0.7408491943849318\n",
            "Iter #16: y_pred 1.0921329902786123: loss: 0.7078670097213877\n",
            "Iter #17: y_pred 1.1241999858610763: loss: 0.6758000141389238\n",
            "Iter #18: y_pred 1.1553299432399835: loss: 0.6446700567600165\n",
            "Iter #19: y_pred 1.185505101647381: loss: 0.6144948983526191\n",
            "Iter #20: y_pred 1.2147118376107076: loss: 0.5852881623892925\n",
            "Iter #21: y_pred 1.2429406423347056: loss: 0.5570593576652945\n",
            "Iter #22: y_pred 1.2701860367729518: loss: 0.5298139632270482\n",
            "Iter #23: y_pred 1.296446433669521: loss: 0.5035535663304791\n",
            "Iter #24: y_pred 1.3217239556708602: loss: 0.47827604432913984\n",
            "Iter #25: y_pred 1.3460242182455233: loss: 0.4539757817544767\n",
            "Iter #26: y_pred 1.3693560856383364: loss: 0.4306439143616636\n",
            "Iter #27: y_pred 1.3917314074600646: loss: 0.4082685925399354\n",
            "Iter #28: y_pred 1.4131647428070897: loss: 0.38683525719291034\n",
            "Iter #29: y_pred 1.4336730780491806: loss: 0.3663269219508194\n",
            "Iter #30: y_pred 1.4532755436450078: loss: 0.34672445635499227\n",
            "Iter #31: y_pred 1.4719931345688557: loss: 0.3280068654311443\n",
            "Iter #32: y_pred 1.489848438177975: loss: 0.31015156182202497\n",
            "Iter #33: y_pred 1.5068653726340864: loss: 0.29313462736591367\n",
            "Iter #34: y_pred 1.5230689383266065: loss: 0.27693106167339354\n",
            "Iter #35: y_pred 1.538484984137377: loss: 0.26151501586262293\n",
            "Iter #36: y_pred 1.5531399898420248: loss: 0.2468600101579752\n",
            "Iter #37: y_pred 1.5670608654636993: loss: 0.23293913453630077\n",
            "Iter #38: y_pred 1.5802747679806675: loss: 0.21972523201933258\n",
            "Iter #39: y_pred 1.5928089354380999: loss: 0.20719106456190017\n",
            "Iter #40: y_pred 1.6046905382229746: loss: 0.19530946177702546\n",
            "Iter #41: y_pred 1.6159465470249987: loss: 0.1840534529750013\n",
            "Iter #42: y_pred 1.6266036168207825: loss: 0.17339638317921757\n",
            "Iter #43: y_pred 1.6366879860778398: loss: 0.16331201392216022\n",
            "Iter #44: y_pred 1.6462253902739443: loss: 0.15377460972605572\n",
            "Iter #45: y_pred 1.6552409887604627: loss: 0.14475901123953738\n",
            "Iter #46: y_pred 1.6637593039605376: loss: 0.13624069603946243\n",
            "Iter #47: y_pred 1.6718041718794512: loss: 0.12819582812054886\n",
            "Iter #48: y_pred 1.6793987029108668: loss: 0.12060129708913325\n",
            "Iter #49: y_pred 1.686565251944918: loss: 0.11343474805508214\n",
            "Iter #50: y_pred 1.6933253968187494: loss: 0.10667460318125066\n",
            "Iter #51: y_pred 1.699699924194076: loss: 0.10030007580592404\n",
            "Iter #52: y_pred 1.7057088219969534: loss: 0.09429117800304665\n",
            "Iter #53: y_pred 1.7113712776099892: loss: 0.08862872239001085\n",
            "Iter #54: y_pred 1.7167056810648456: loss: 0.08329431893515449\n",
            "Iter #55: y_pred 1.7217296325414804: loss: 0.07827036745851967\n",
            "Iter #56: y_pred 1.7264599535389993: loss: 0.07354004646100076\n",
            "Iter #57: y_pred 1.7309127011401988: loss: 0.06908729885980125\n",
            "Iter #58: y_pred 1.7351031848471579: loss: 0.06489681515284218\n",
            "Iter #59: y_pred 1.7390459855180396: loss: 0.06095401448196047\n",
            "Iter #60: y_pred 1.7427549759851646: loss: 0.057245024014835444\n",
            "Iter #61: y_pred 1.7462433429812245: loss: 0.05375665701877552\n",
            "Iter #62: y_pred 1.7495236100440095: loss: 0.050476389955990575\n",
            "Iter #63: y_pred 1.7526076611102195: loss: 0.04739233888978056\n",
            "Iter #64: y_pred 1.7555067645458284: loss: 0.04449323545417161\n",
            "Iter #65: y_pred 1.7582315973941325: loss: 0.04176840260586756\n",
            "Iter #66: y_pred 1.7607922696531624: loss: 0.039207730346837666\n",
            "Iter #67: y_pred 1.7631983484217237: loss: 0.036801651578276307\n",
            "Iter #68: y_pred 1.7654588817781245: loss: 0.034541118221875555\n",
            "Iter #69: y_pred 1.7675824222777847: loss: 0.03241757772221532\n",
            "Iter #70: y_pred 1.769577049975673: loss: 0.030422950024326934\n",
            "Iter #71: y_pred 1.771450394896963: loss: 0.028549605103036946\n",
            "Iter #72: y_pred 1.7732096588947255: loss: 0.026790341105274562\n",
            "Iter #73: y_pred 1.774861636846979: loss: 0.025138363153021093\n",
            "Iter #74: y_pred 1.7764127371572316: loss: 0.023587262842768464\n",
            "Iter #75: y_pred 1.7778690015329075: loss: 0.022130998467092544\n",
            "Iter #76: y_pred 1.7792361240248986: loss: 0.02076387597510143\n",
            "Iter #77: y_pred 1.7805194693191018: loss: 0.019480530680898278\n",
            "Iter #78: y_pred 1.7817240902772846: loss: 0.018275909722715422\n",
            "Iter #79: y_pred 1.7828547447301204: loss: 0.017145255269879645\n",
            "Iter #80: y_pred 1.7839159115298464: loss: 0.016084088470153635\n",
            "Iter #81: y_pred 1.7849118058738256: loss: 0.015088194126174459\n",
            "Iter #82: y_pred 1.785846393913453: loss: 0.014153606086547033\n",
            "Iter #83: y_pred 1.7867234066653805: loss: 0.013276593334619502\n",
            "Iter #84: y_pred 1.7875463532440663: loss: 0.012453646755933745\n",
            "Iter #85: y_pred 1.7883185334362184: loss: 0.011681466563781662\n",
            "Iter #86: y_pred 1.7890430496388667: loss: 0.01095695036113331\n",
            "Iter #87: y_pred 1.7897228181836415: loss: 0.010277181816358505\n",
            "Iter #88: y_pred 1.7903605800703621: loss: 0.009639419929637905\n",
            "Iter #89: y_pred 1.79095891113334: loss: 0.009041088866660107\n",
            "Iter #90: y_pred 1.791520231663874: loss: 0.008479768336125959\n",
            "Iter #91: y_pred 1.7920468155123246: loss: 0.007953184487675458\n",
            "Iter #92: y_pred 1.7925407986929023: loss: 0.0074592013070977625\n",
            "Iter #93: y_pred 1.7930041875139484: loss: 0.0069958124860516335\n",
            "Iter #94: y_pred 1.79343886625601: loss: 0.006561133743990144\n",
            "Iter #95: y_pred 1.7938466044194783: loss: 0.006153395580521748\n",
            "Iter #96: y_pred 1.7942290635629397: loss: 0.005770936437060392\n",
            "Iter #97: y_pred 1.7945878037527445: loss: 0.00541219624725553\n",
            "Iter #98: y_pred 1.7949242896435964: loss: 0.00507571035640364\n",
            "Iter #99: y_pred 1.7952398962092577: loss: 0.0047601037907423205\n",
            "Iter #100: y_pred 1.7955359141417324: loss: 0.00446408585826763\n",
            "Iter #101: y_pred 1.7958135549365493: loss: 0.004186445063450783\n",
            "Iter #102: y_pred 1.7960739556810235: loss: 0.003926044318976585\n",
            "Iter #103: y_pred 1.7963181835616495: loss: 0.003681816438350527\n",
            "Iter #104: y_pred 1.7965472401060396: loss: 0.003452759893960433\n",
            "Iter #105: y_pred 1.7967620651741159: loss: 0.0032379348258841922\n",
            "Iter #106: y_pred 1.7969635407125617: loss: 0.0030364592874383423\n",
            "Iter #107: y_pred 1.7971524942858599: loss: 0.002847505714140164\n",
            "Iter #108: y_pred 1.7973297023965797: loss: 0.002670297603420302\n",
            "Iter #109: y_pred 1.797495893606938: loss: 0.002504106393061978\n",
            "Iter #110: y_pred 1.7976517514730475: loss: 0.00234824852695259\n",
            "Iter #111: y_pred 1.7977979173026548: loss: 0.002202082697345231\n",
            "Iter #112: y_pred 1.7979349927466213: loss: 0.00206500725337877\n",
            "Iter #113: y_pred 1.7980635422338285: loss: 0.0019364577661715732\n",
            "Iter #114: y_pred 1.7981840952586885: loss: 0.001815904741311547\n",
            "Iter #115: y_pred 1.7982971485299186: loss: 0.0017028514700814235\n",
            "Iter #116: y_pred 1.7984031679887718: loss: 0.0015968320112282886\n",
            "Iter #117: y_pred 1.798502590704461: loss: 0.0014974092955390983\n",
            "Iter #118: y_pred 1.7985958266540698: loss: 0.0014041733459302375\n",
            "Iter #119: y_pred 1.7986832603938423: loss: 0.0013167396061577463\n",
            "Iter #120: y_pred 1.7987652526283424: loss: 0.0012347473716576296\n",
            "Iter #121: y_pred 1.7988421416836091: loss: 0.001157858316390925\n",
            "Iter #122: y_pred 1.798914244890077: loss: 0.0010857551099230367\n",
            "Iter #123: y_pred 1.7989818598806961: loss: 0.0010181401193039008\n",
            "Iter #124: y_pred 1.7990452658093747: loss: 0.000954734190625306\n",
            "Iter #125: y_pred 1.7991047244945615: loss: 0.000895275505438553\n",
            "Iter #126: y_pred 1.7991604814925055: loss: 0.0008395185074945299\n",
            "Iter #127: y_pred 1.7992127671044686: loss: 0.0007872328955313979\n",
            "Iter #128: y_pred 1.799261797321895: loss: 0.0007382026781050932\n",
            "Iter #129: y_pred 1.7993077747133348: loss: 0.0006922252866652379\n",
            "Iter #130: y_pred 1.799350889256658: loss: 0.0006491107433419518\n",
            "Iter #131: y_pred 1.7993913191199102: loss: 0.0006086808800898069\n",
            "Iter #132: y_pred 1.7994292313939486: loss: 0.0005707686060514305\n",
            "Iter #133: y_pred 1.7994647827798114: loss: 0.0005352172201886773\n",
            "Iter #134: y_pred 1.799498120233592: loss: 0.000501879766408031\n",
            "Iter #135: y_pred 1.7995293815714317: loss: 0.0004706184285683257\n",
            "Iter #136: y_pred 1.799558696037076: loss: 0.0004413039629240778\n",
            "Iter #137: y_pred 1.7995861848342953: loss: 0.0004138151657047118\n",
            "Iter #138: y_pred 1.7996119616263375: loss: 0.0003880383736625248\n",
            "Iter #139: y_pred 1.7996361330044324: loss: 0.0003638669955676743\n",
            "Iter #140: y_pred 1.7996587989272692: loss: 0.00034120107273083455\n",
            "Iter #141: y_pred 1.7996800531332222: loss: 0.0003199468667778316\n",
            "Iter #142: y_pred 1.799699983527016: loss: 0.00030001647298405487\n",
            "Iter #143: y_pred 1.7997186725424061: loss: 0.0002813274575939051\n",
            "Iter #144: y_pred 1.7997361974823547: loss: 0.0002638025176453507\n",
            "Iter #145: y_pred 1.7997526308380933: loss: 0.00024736916190670755\n",
            "Iter #146: y_pred 1.799768040588381: loss: 0.00023195941161913147\n",
            "Iter #147: y_pred 1.799782490480178: loss: 0.00021750951982202338\n",
            "Iter #148: y_pred 1.7997960402918904: loss: 0.0002039597081096023\n",
            "Iter #149: y_pred 1.799808746080261: loss: 0.00019125391973906503\n",
            "Iter #150: y_pred 1.799820660411923: loss: 0.00017933958807714312\n",
            "Iter #151: y_pred 1.799831832580562: loss: 0.0001681674194380367\n",
            "Iter #152: y_pred 1.7998423088105866: loss: 0.00015769118941344473\n",
            "Iter #153: y_pred 1.7998521324481318: loss: 0.00014786755186824152\n",
            "Iter #154: y_pred 1.799861344140195: loss: 0.00013865585980510353\n",
            "Iter #155: y_pred 1.7998699820026305: loss: 0.0001300179973695581\n",
            "Iter #156: y_pred 1.799878081777698: loss: 0.00012191822230200877\n",
            "Iter #157: y_pred 1.7998856769818123: loss: 0.00011432301818770085\n",
            "Iter #158: y_pred 1.799892799044106: loss: 0.00010720095589400458\n",
            "Iter #159: y_pred 1.7998994774363692: loss: 0.00010052256363080048\n",
            "Iter #160: y_pred 1.7999057397949083: loss: 9.426020509173405e-05\n",
            "Iter #161: y_pred 1.7999116120348224: loss: 8.838796517762759e-05\n",
            "Iter #162: y_pred 1.7999171184571694: loss: 8.288154283064841e-05\n",
            "Iter #163: y_pred 1.799922281849463: loss: 7.77181505371427e-05\n",
            "Iter #164: y_pred 1.799927123579917: loss: 7.287642008302342e-05\n",
            "Iter #165: y_pred 1.799931663685822: loss: 6.833631417801911e-05\n",
            "Iter #166: y_pred 1.7999359209564247: loss: 6.40790435753047e-05\n",
            "Iter #167: y_pred 1.7999399130106466: loss: 6.008698935344725e-05\n",
            "Iter #168: y_pred 1.7999436563699636: loss: 5.6343630036481684e-05\n",
            "Iter #169: y_pred 1.7999471665267535: loss: 5.283347324658294e-05\n",
            "Iter #170: y_pred 1.7999504580083818: loss: 4.954199161821826e-05\n",
            "Iter #171: y_pred 1.7999535444373065: loss: 4.6455562693559216e-05\n",
            "Iter #172: y_pred 1.7999564385874307: loss: 4.356141256933732e-05\n",
            "Iter #173: y_pred 1.7999591524369567: loss: 4.084756304334469e-05\n",
            "Iter #174: y_pred 1.7999616972179413: loss: 3.830278205874116e-05\n",
            "Iter #175: y_pred 1.7999640834627697: loss: 3.5916537230340495e-05\n",
            "Iter #176: y_pred 1.7999663210477341: loss: 3.367895226591422e-05\n",
            "Iter #177: y_pred 1.7999684192338996: loss: 3.1580766100436364e-05\n",
            "Iter #178: y_pred 1.7999703867054244: loss: 2.961329457562556e-05\n",
            "Iter #179: y_pred 1.7999722316054936: loss: 2.776839450646662e-05\n",
            "Iter #180: y_pred 1.7999739615700154: loss: 2.603842998460948e-05\n",
            "Iter #181: y_pred 1.7999755837592204: loss: 2.4416240779645548e-05\n",
            "Iter #182: y_pred 1.7999771048872897: loss: 2.289511271036382e-05\n",
            "Iter #183: y_pred 1.7999785312501404: loss: 2.146874985964331e-05\n",
            "Iter #184: y_pred 1.7999798687514796: loss: 2.0131248520405265e-05\n",
            "Iter #185: y_pred 1.7999811229272356: loss: 1.8877072764489355e-05\n",
            "Iter #186: y_pred 1.7999822989684693: loss: 1.770103153075908e-05\n",
            "Iter #187: y_pred 1.799983401742853: loss: 1.659825714694918e-05\n",
            "Iter #188: y_pred 1.7999844358148207: loss: 1.5564185179339773e-05\n",
            "Iter #189: y_pred 1.799985405464453: loss: 1.459453554697454e-05\n",
            "Iter #190: y_pred 1.7999863147051942: loss: 1.3685294805831916e-05\n",
            "Iter #191: y_pred 1.799987167300459: loss: 1.283269954099886e-05\n",
            "Iter #192: y_pred 1.799987966779212: loss: 1.2033220788021382e-05\n",
            "Iter #193: y_pred 1.7999887164505695: loss: 1.1283549430585182e-05\n",
            "Iter #194: y_pred 1.799989419417496: loss: 1.0580582504138292e-05\n",
            "Iter #195: y_pred 1.799990078589648: loss: 9.921410351942939e-06\n",
            "Iter #196: y_pred 1.7999906966954144: loss: 9.303304585595029e-06\n",
            "Iter #197: y_pred 1.7999912762932098: loss: 8.723706790281227e-06\n",
            "Iter #198: y_pred 1.799991819782063: loss: 8.180217937026057e-06\n",
            "Iter #199: y_pred 1.7999923294115467: loss: 7.670588453301264e-06\n",
            "Iter #200: y_pred 1.7999928072910878: loss: 7.192708912251433e-06\n",
            "Iter #201: y_pred 1.7999932553986981: loss: 6.744601301900133e-06\n",
            "Iter #202: y_pred 1.7999936755891608: loss: 6.3244108392535026e-06\n",
            "Iter #203: y_pred 1.799994069601709: loss: 5.93039829110964e-06\n",
            "Iter #204: y_pred 1.7999944390672216: loss: 5.560932778481131e-06\n",
            "Iter #205: y_pred 1.7999947855149756: loss: 5.214485024440663e-06\n",
            "Iter #206: y_pred 1.7999951103789755: loss: 4.88962102451751e-06\n",
            "Iter #207: y_pred 1.7999954150038868: loss: 4.584996113221607e-06\n",
            "Iter #208: y_pred 1.799995700650605: loss: 4.2993493949428085e-06\n",
            "Iter #209: y_pred 1.7999959685014708: loss: 4.031498529233346e-06\n",
            "Iter #210: y_pred 1.7999962196651662: loss: 3.7803348338361076e-06\n",
            "Iter #211: y_pred 1.7999964551813026: loss: 3.5448186974651463e-06\n",
            "Iter #212: y_pred 1.7999966760247241: loss: 3.323975275915103e-06\n",
            "Iter #213: y_pred 1.799996883109542: loss: 3.116890457954824e-06\n",
            "Iter #214: y_pred 1.799997077292919: loss: 2.9227070810211586e-06\n",
            "Iter #215: y_pred 1.7999972593786162: loss: 2.740621383834352e-06\n",
            "Iter #216: y_pred 1.7999974301203217: loss: 2.569879678393505e-06\n",
            "Iter #217: y_pred 1.7999975902247667: loss: 2.409775233358502e-06\n",
            "Iter #218: y_pred 1.799997740354656: loss: 2.2596453439494013e-06\n",
            "Iter #219: y_pred 1.7999978811314055: loss: 2.1188685945805474e-06\n",
            "Iter #220: y_pred 1.7999980131377178: loss: 1.9868622822549753e-06\n",
            "Iter #221: y_pred 1.7999981369199936: loss: 1.8630800064922681e-06\n",
            "Iter #222: y_pred 1.7999982529905918: loss: 1.7470094082483456e-06\n",
            "Iter #223: y_pred 1.799998361829952: loss: 1.6381700480572192e-06\n",
            "Iter #224: y_pred 1.7999984638885824: loss: 1.5361114176215551e-06\n",
            "Iter #225: y_pred 1.7999985595889243: loss: 1.4404110757482158e-06\n",
            "Iter #226: y_pred 1.7999986493271007: loss: 1.3506728993029071e-06\n",
            "Iter #227: y_pred 1.799998733474556: loss: 1.2665254440769047e-06\n",
            "Iter #228: y_pred 1.7999988123795947: loss: 1.187620405351808e-06\n",
            "Iter #229: y_pred 1.7999988863688199: loss: 1.1136311801607235e-06\n",
            "Iter #230: y_pred 1.7999989557484892: loss: 1.044251510817773e-06\n",
            "Iter #231: y_pred 1.7999990208057788: loss: 9.791942212622473e-07\n",
            "Iter #232: y_pred 1.7999990818099745: loss: 9.181900255672559e-07\n",
            "Iter #233: y_pred 1.7999991390135854: loss: 8.609864146080781e-07\n",
            "Iter #234: y_pred 1.7999991926533896: loss: 8.073466104541183e-07\n",
            "Iter #235: y_pred 1.799999242951413: loss: 7.570485871521981e-07\n",
            "Iter #236: y_pred 1.79999929011585: loss: 7.098841501296249e-07\n",
            "Iter #237: y_pred 1.799999334341924: loss: 6.656580759933917e-07\n",
            "Iter #238: y_pred 1.7999993758126962: loss: 6.241873038437262e-07\n",
            "Iter #239: y_pred 1.7999994146998228: loss: 5.8530017721381e-07\n",
            "Iter #240: y_pred 1.7999994511642656: loss: 5.488357344152206e-07\n",
            "Iter #241: y_pred 1.7999994853569594: loss: 5.146430406277602e-07\n",
            "Iter #242: y_pred 1.7999995174194343: loss: 4.825805657304727e-07\n",
            "Iter #243: y_pred 1.7999995474844044: loss: 4.5251559566139576e-07\n",
            "Iter #244: y_pred 1.7999995756763145: loss: 4.243236855216992e-07\n",
            "Iter #245: y_pred 1.7999996021118572: loss: 3.978881428778891e-07\n",
            "Iter #246: y_pred 1.799999626900455: loss: 3.7309954503683684e-07\n",
            "Iter #247: y_pred 1.7999996501447126: loss: 3.4985528740705263e-07\n",
            "Iter #248: y_pred 1.7999996719408438: loss: 3.280591562848656e-07\n",
            "Iter #249: y_pred 1.7999996923790667: loss: 3.0762093339298247e-07\n",
            "Iter #250: y_pred 1.79999971154398: loss: 2.884560199589714e-07\n",
            "Iter #251: y_pred 1.7999997295149115: loss: 2.704850885493215e-07\n",
            "Iter #252: y_pred 1.7999997463662463: loss: 2.536337537772937e-07\n",
            "Iter #253: y_pred 1.7999997621677362: loss: 2.378322638829644e-07\n",
            "Iter #254: y_pred 1.7999997769847864: loss: 2.2301521362955157e-07\n",
            "Iter #255: y_pred 1.7999997908787284: loss: 2.091212716326396e-07\n",
            "Iter #256: y_pred 1.7999998039070721: loss: 1.9609292789546373e-07\n",
            "Iter #257: y_pred 1.7999998161237445: loss: 1.8387625555504883e-07\n",
            "Iter #258: y_pred 1.7999998275793132: loss: 1.7242068683920309e-07\n",
            "Iter #259: y_pred 1.7999998383211948: loss: 1.6167880523276779e-07\n",
            "Iter #260: y_pred 1.7999998483938526: loss: 1.5160614741382972e-07\n",
            "Iter #261: y_pred 1.7999998578389798: loss: 1.421610202889667e-07\n",
            "Iter #262: y_pred 1.7999998666956716: loss: 1.3330432846458962e-07\n",
            "Iter #263: y_pred 1.7999998750005881: loss: 1.249994119323361e-07\n",
            "Iter #264: y_pred 1.7999998827881043: loss: 1.1721189574487312e-07\n",
            "Iter #265: y_pred 1.7999998900904555: loss: 1.0990954457668067e-07\n",
            "Iter #266: y_pred 1.7999998969378668: loss: 1.0306213327204716e-07\n",
            "Iter #267: y_pred 1.7999999033586815: loss: 9.664131850328772e-08\n",
            "Iter #268: y_pred 1.7999999093794767: loss: 9.062052330754966e-08\n"
          ]
        }
      ],
      "source": [
        "model = build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "mR5WBYto4NV2",
        "outputId": "993c5bc2-eadb-410a-8a8f-c01ed2d4a13a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>train_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.322249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.279680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.237540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.195837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.154590</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   iter  train_loss\n",
              "0     1    1.322249\n",
              "1     2    1.279680\n",
              "2     3    1.237540\n",
              "3     4    1.195837\n",
              "4     5    1.154590"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = pd.DataFrame({'train_loss': model['train_loss']})\n",
        "loss = loss.reset_index().rename(columns={'index': 'iter'})\n",
        "loss['iter'] += 1\n",
        "loss.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "colab_type": "code",
        "id": "-UV2RRs_5H7v",
        "outputId": "c0912c24-8673-44e4-860a-e26712bf5fe8"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[1;32m      3\u001b[0m fig \u001b[38;5;241m=\u001b[39m go\u001b[38;5;241m.\u001b[39mFigure(data\u001b[38;5;241m=\u001b[39mgo\u001b[38;5;241m.\u001b[39mScatter(x\u001b[38;5;241m=\u001b[39mloss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m'\u001b[39m], y\u001b[38;5;241m=\u001b[39mloss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarkers+lines\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/ML_DL_Python_Comarch/.venv/lib/python3.9/site-packages/plotly/basedatatypes.py:3420\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3388\u001b[0m \u001b[38;5;124;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3389\u001b[0m \u001b[38;5;124;03mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3416\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m   3417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3418\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m-> 3420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/ML_DL_Python_Comarch/.venv/lib/python3.9/site-packages/plotly/io/_renderers.py:415\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m     )\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    419\u001b[0m display_jupyter_version_warnings()\n\u001b[1;32m    421\u001b[0m ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
          ]
        }
      ],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=go.Scatter(x=loss['iter'], y=loss['train_loss'], mode='markers+lines'))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2s66fHa43LDk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.7999999093794767"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict(X, model['W1'], model['W2'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
